# zkMLaaS Implementation Guide

**Week-by-Week Development Plan**

---

## Pre-Development Checklist

### Technical Prerequisites

- [ ] JOLT-Atlas binary compiled and tested
- [ ] ONNX Runtime installed (onnxruntime-node)
- [ ] PostgreSQL database ready
- [ ] Base mainnet RPC access (Alchemy/Infura)
- [ ] USDC contract address verified (Base mainnet)
- [ ] Groth16 circuit tools (circom, snarkjs)
- [ ] IPFS node or Infura IPFS API
- [ ] Domain registered (zkmlaa.s or similar)

### Repository Setup

```bash
# Create new repository
gh repo create hshadab/zkMLaaS --public --description "zkML-as-a-Service Platform for X402 Bazaar"

# Clone and initialize
git clone https://github.com/hshadab/zkMLaaS.git
cd zkMLaaS

# Copy planning docs
cp ../zkml-erc8004/ZKMLAAAS_*.md ./

# Initialize project
npm init -y
npm install express pg redis ethers onnxruntime-node snarkjs ipfs-http-client
npm install --save-dev typescript @types/node @types/express jest

# Create structure
mkdir -p src/{api,services,middleware,database,utils}
mkdir -p models circuits ui scripts tests docs
mkdir -p .github/workflows

# Initial commit
git add .
git commit -m "feat: initialize zkMLaaS platform"
git push origin main
```

---

## Week 1: Foundation + Payment Verification

### Day 1-2: Backend Setup

**Goal:** Express server with PostgreSQL

**Files to Create:**

#### `src/server.js`
```javascript
import express from 'express';
import cors from 'cors';
import { config } from './config.js';
import { logger } from './utils/logger.js';
import { errorHandler } from './middleware/errorHandler.js';

const app = express();

// Middleware
app.use(cors());
app.use(express.json({ limit: '10mb' }));
app.use(logger.httpLogger());

// Health check
app.get('/health', (req, res) => {
  res.json({
    status: 'healthy',
    timestamp: new Date().toISOString(),
    version: '0.1.0'
  });
});

// API routes (will add later)
// app.use('/api', apiRoutes);

// Error handling
app.use(errorHandler);

// Start server
const PORT = config.port || 9100;
app.listen(PORT, () => {
  logger.info(`üöÄ zkMLaaS running on port ${PORT}`);
  logger.info(`   Health: http://localhost:${PORT}/health`);
});

export { app };
```

#### `src/config.js`
```javascript
import dotenv from 'dotenv';
dotenv.config();

export const config = {
  // Server
  port: process.env.PORT || 9100,
  nodeEnv: process.env.NODE_ENV || 'development',

  // Database
  database: {
    url: process.env.DATABASE_URL,
    pool: {
      min: 2,
      max: 10
    }
  },

  // Redis
  redis: {
    url: process.env.REDIS_URL || 'redis://localhost:6379'
  },

  // Blockchain (Base Mainnet)
  blockchain: {
    rpcUrl: process.env.RPC_URL,
    chainId: 8453,
    usdcAddress: '0x833589fCD6eDb6E08f4c7C32D4f71b54bdA02913',
    platformWallet: process.env.PLATFORM_WALLET,
    privateKey: process.env.PLATFORM_PRIVATE_KEY
  },

  // JOLT
  jolt: {
    binaryPath: process.env.JOLT_BINARY_PATH,
    timeout: 60000
  },

  // Groth16
  groth16: {
    wasmPath: './circuits/jolt-attestation-wrapper.wasm',
    zkeyPath: './circuits/jolt-attestation-wrapper.zkey'
  },

  // IPFS
  ipfs: {
    apiUrl: process.env.IPFS_API_URL || 'https://ipfs.infura.io:5001',
    gateway: process.env.IPFS_GATEWAY || 'https://gateway.ipfs.io'
  }
};
```

#### `src/database/schema.sql`
```sql
-- Verifications table (inference + proof records)
CREATE TABLE verifications (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    model_id VARCHAR(100) NOT NULL,
    tx_hash VARCHAR(66) NOT NULL UNIQUE,
    payment_amount DECIMAL(18,6) NOT NULL,
    payer_address VARCHAR(42) NOT NULL,

    input_data TEXT NOT NULL,
    input_hash VARCHAR(66) NOT NULL,
    output_data TEXT NOT NULL,
    output_hash VARCHAR(66) NOT NULL,

    jolt_proof_hash VARCHAR(66) NOT NULL,
    groth16_proof_ipfs VARCHAR(100),

    inference_time_ms INTEGER NOT NULL,
    jolt_time_ms INTEGER NOT NULL,
    groth16_time_ms INTEGER NOT NULL,
    total_time_ms INTEGER NOT NULL,

    status VARCHAR(20) NOT NULL,  -- 'success', 'failed', 'pending'
    error_message TEXT,

    verified_at TIMESTAMP NOT NULL DEFAULT NOW(),
    created_at TIMESTAMP NOT NULL DEFAULT NOW()
);

-- Indexes
CREATE INDEX idx_verifications_tx_hash ON verifications(tx_hash);
CREATE INDEX idx_verifications_model_id ON verifications(model_id);
CREATE INDEX idx_verifications_payer ON verifications(payer_address);
CREATE INDEX idx_verifications_created_at ON verifications(created_at DESC);
CREATE INDEX idx_verifications_status ON verifications(status);

-- Models table (model registry)
CREATE TABLE models (
    id VARCHAR(100) PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    description TEXT,
    developer_address VARCHAR(42) NOT NULL,

    onnx_path TEXT NOT NULL,
    onnx_hash VARCHAR(66) NOT NULL,
    onnx_size_mb DECIMAL(10,2) NOT NULL,

    input_type VARCHAR(50) NOT NULL,  -- 'text', 'feature-vector', 'image'
    output_type VARCHAR(50) NOT NULL,  -- 'classification', 'regression', 'embedding'
    max_input_size INTEGER NOT NULL,

    price DECIMAL(18,6) NOT NULL,
    currency VARCHAR(10) NOT NULL DEFAULT 'USDC',
    revenue_share DECIMAL(5,4) NOT NULL DEFAULT 0.70,

    accuracy_percent INTEGER,
    avg_inference_ms INTEGER,
    avg_proof_ms INTEGER,
    avg_total_ms INTEGER,

    total_calls INTEGER DEFAULT 0,
    monthly_calls INTEGER DEFAULT 0,
    total_revenue DECIMAL(18,6) DEFAULT 0,

    status VARCHAR(20) NOT NULL DEFAULT 'pending',  -- 'pending', 'active', 'suspended'

    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMP NOT NULL DEFAULT NOW()
);

-- Indexes
CREATE INDEX idx_models_developer ON models(developer_address);
CREATE INDEX idx_models_status ON models(status);
CREATE INDEX idx_models_price ON models(price);

-- Developers table
CREATE TABLE developers (
    address VARCHAR(42) PRIMARY KEY,
    name VARCHAR(255),
    email VARCHAR(255),

    models_submitted INTEGER DEFAULT 0,
    models_active INTEGER DEFAULT 0,

    total_revenue DECIMAL(18,6) DEFAULT 0,
    pending_payout DECIMAL(18,6) DEFAULT 0,
    total_paid DECIMAL(18,6) DEFAULT 0,

    api_key VARCHAR(64) UNIQUE,

    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMP NOT NULL DEFAULT NOW()
);

-- Payment transactions (used to prevent replay attacks)
CREATE TABLE payment_txs (
    tx_hash VARCHAR(66) PRIMARY KEY,
    from_address VARCHAR(42) NOT NULL,
    to_address VARCHAR(42) NOT NULL,
    amount DECIMAL(18,6) NOT NULL,
    currency VARCHAR(10) NOT NULL,
    network VARCHAR(20) NOT NULL,
    block_number INTEGER NOT NULL,
    confirmed_at TIMESTAMP NOT NULL,
    used_for_verification UUID REFERENCES verifications(id),
    created_at TIMESTAMP NOT NULL DEFAULT NOW()
);

-- Indexes
CREATE INDEX idx_payment_txs_from ON payment_txs(from_address);
CREATE INDEX idx_payment_txs_block ON payment_txs(block_number);

-- Revenue splits (track developer earnings)
CREATE TABLE revenue_splits (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    verification_id UUID NOT NULL REFERENCES verifications(id),
    model_id VARCHAR(100) NOT NULL REFERENCES models(id),
    developer_address VARCHAR(42) NOT NULL REFERENCES developers(address),

    total_amount DECIMAL(18,6) NOT NULL,
    developer_share DECIMAL(18,6) NOT NULL,
    platform_share DECIMAL(18,6) NOT NULL,
    revenue_share_percent DECIMAL(5,4) NOT NULL,

    payout_status VARCHAR(20) NOT NULL DEFAULT 'pending',  -- 'pending', 'paid', 'failed'
    payout_tx_hash VARCHAR(66),
    paid_at TIMESTAMP,

    created_at TIMESTAMP NOT NULL DEFAULT NOW()
);

-- Indexes
CREATE INDEX idx_revenue_splits_developer ON revenue_splits(developer_address);
CREATE INDEX idx_revenue_splits_payout_status ON revenue_splits(payout_status);
CREATE INDEX idx_revenue_splits_created_at ON revenue_splits(created_at DESC);
```

#### `src/database/db.js`
```javascript
import pg from 'pg';
import { config } from '../config.js';
import { logger } from '../utils/logger.js';

const { Pool } = pg;

export const pool = new Pool({
  connectionString: config.database.url,
  ...config.database.pool
});

// Test connection
pool.on('connect', () => {
  logger.info('‚úÖ PostgreSQL connected');
});

pool.on('error', (err) => {
  logger.error('‚ùå PostgreSQL error:', err);
});

// Query helper
export async function query(text, params) {
  const start = Date.now();
  try {
    const result = await pool.query(text, params);
    const duration = Date.now() - start;
    logger.debug(`Query executed in ${duration}ms: ${text}`);
    return result;
  } catch (error) {
    logger.error('Query error:', { text, params, error: error.message });
    throw error;
  }
}
```

**Test:**
```bash
# Create .env file
cat > .env <<EOF
PORT=9100
NODE_ENV=development
DATABASE_URL=postgresql://user:pass@localhost:5432/zkmlaaas
REDIS_URL=redis://localhost:6379
RPC_URL=https://base-mainnet.g.alchemy.com/v2/YOUR_KEY
PLATFORM_WALLET=0x...
PLATFORM_PRIVATE_KEY=0x...
JOLT_BINARY_PATH=/path/to/zkml-jolt-core
EOF

# Run database migrations
psql $DATABASE_URL < src/database/schema.sql

# Start server
npm start

# Test health endpoint
curl http://localhost:9100/health
```

### Day 3-4: X402 Payment Verification

**Goal:** Verify USDC transfers on Base

**Files to Create:**

#### `src/services/X402PaymentVerifier.js`
```javascript
import { ethers } from 'ethers';
import { config } from '../config.js';
import { logger } from '../utils/logger.js';
import { query } from '../database/db.js';

// USDC ABI (minimal - just Transfer event)
const USDC_ABI = [
  'event Transfer(address indexed from, address indexed to, uint256 value)',
  'function decimals() view returns (uint8)'
];

export class X402PaymentVerifier {
  constructor() {
    this.provider = new ethers.JsonRpcProvider(config.blockchain.rpcUrl);
    this.usdcContract = new ethers.Contract(
      config.blockchain.usdcAddress,
      USDC_ABI,
      this.provider
    );
    this.decimals = 6; // USDC has 6 decimals
  }

  /**
   * Verify payment transaction
   * @param {string} txHash - Transaction hash
   * @param {string} expectedAmount - Expected amount in USDC (e.g., "0.25")
   * @returns {Promise<{valid: boolean, amount: string, from: string, timestamp: string}>}
   */
  async verifyPayment(txHash, expectedAmount) {
    try {
      logger.info(`üîç Verifying payment: ${txHash}`);

      // Check if already used (prevent replay)
      const existingTx = await query(
        'SELECT * FROM payment_txs WHERE tx_hash = $1',
        [txHash]
      );

      if (existingTx.rows.length > 0) {
        logger.warn(`‚ö†Ô∏è  Payment already used: ${txHash}`);
        return {
          valid: false,
          error: 'Payment already used (replay attack prevented)',
          code: 'PAYMENT_ALREADY_USED'
        };
      }

      // Fetch transaction receipt
      const receipt = await this.provider.getTransactionReceipt(txHash);

      if (!receipt) {
        logger.warn(`‚ö†Ô∏è  Transaction not found: ${txHash}`);
        return {
          valid: false,
          error: 'Transaction not found',
          code: 'TX_NOT_FOUND'
        };
      }

      if (receipt.status !== 1) {
        logger.warn(`‚ö†Ô∏è  Transaction failed: ${txHash}`);
        return {
          valid: false,
          error: 'Transaction failed',
          code: 'TX_FAILED'
        };
      }

      // Check confirmations (require at least 3)
      const currentBlock = await this.provider.getBlockNumber();
      const confirmations = currentBlock - receipt.blockNumber;

      if (confirmations < 3) {
        logger.warn(`‚ö†Ô∏è  Insufficient confirmations: ${confirmations}/3`);
        return {
          valid: false,
          error: `Insufficient confirmations (${confirmations}/3)`,
          code: 'INSUFFICIENT_CONFIRMATIONS'
        };
      }

      // Parse Transfer event
      const transferEvent = receipt.logs
        .filter(log => log.address.toLowerCase() === config.blockchain.usdcAddress.toLowerCase())
        .map(log => {
          try {
            return this.usdcContract.interface.parseLog(log);
          } catch {
            return null;
          }
        })
        .find(event => event && event.name === 'Transfer');

      if (!transferEvent) {
        logger.warn(`‚ö†Ô∏è  No USDC Transfer event found`);
        return {
          valid: false,
          error: 'No USDC transfer found',
          code: 'NO_TRANSFER_EVENT'
        };
      }

      // Verify recipient
      const to = transferEvent.args.to.toLowerCase();
      const expectedRecipient = config.blockchain.platformWallet.toLowerCase();

      if (to !== expectedRecipient) {
        logger.warn(`‚ö†Ô∏è  Wrong recipient: ${to} (expected ${expectedRecipient})`);
        return {
          valid: false,
          error: `Wrong recipient (expected ${expectedRecipient})`,
          code: 'WRONG_RECIPIENT'
        };
      }

      // Verify amount
      const amountBigInt = transferEvent.args.value;
      const amount = ethers.formatUnits(amountBigInt, this.decimals);
      const expectedAmountFloat = parseFloat(expectedAmount);
      const actualAmountFloat = parseFloat(amount);

      if (actualAmountFloat < expectedAmountFloat) {
        logger.warn(`‚ö†Ô∏è  Insufficient amount: ${amount} < ${expectedAmount}`);
        return {
          valid: false,
          error: `Insufficient amount (${amount} < ${expectedAmount})`,
          code: 'INSUFFICIENT_AMOUNT'
        };
      }

      // Get block timestamp
      const block = await this.provider.getBlock(receipt.blockNumber);
      const timestamp = new Date(block.timestamp * 1000).toISOString();

      // Store in database (mark as used)
      await query(
        `INSERT INTO payment_txs
         (tx_hash, from_address, to_address, amount, currency, network, block_number, confirmed_at)
         VALUES ($1, $2, $3, $4, $5, $6, $7, $8)`,
        [
          txHash,
          transferEvent.args.from,
          to,
          amount,
          'USDC',
          'Base',
          receipt.blockNumber,
          timestamp
        ]
      );

      logger.info(`‚úÖ Payment verified: ${amount} USDC from ${transferEvent.args.from}`);

      return {
        valid: true,
        amount,
        from: transferEvent.args.from,
        to,
        timestamp,
        blockNumber: receipt.blockNumber,
        confirmations
      };

    } catch (error) {
      logger.error(`‚ùå Payment verification failed:`, error);
      return {
        valid: false,
        error: error.message,
        code: 'VERIFICATION_ERROR'
      };
    }
  }

  /**
   * Mark payment as used for specific verification
   */
  async markUsed(txHash, verificationId) {
    await query(
      'UPDATE payment_txs SET used_for_verification = $1 WHERE tx_hash = $2',
      [verificationId, txHash]
    );
  }
}
```

**Test:**
```javascript
// test-payment-verification.js
import { X402PaymentVerifier } from './src/services/X402PaymentVerifier.js';

const verifier = new X402PaymentVerifier();

// Test with a real Base mainnet USDC transfer
const testTxHash = '0x...';  // Your test transaction
const result = await verifier.verifyPayment(testTxHash, '0.25');

console.log('Payment Verification Result:', result);
```

### Day 5: Model Registry

**Goal:** Store and retrieve model metadata

**Files to Create:**

#### `src/services/ModelRegistry.js`
```javascript
import { query } from '../database/db.js';
import { logger } from '../utils/logger.js';
import crypto from 'crypto';

export class ModelRegistry {
  /**
   * Register a new model
   */
  async registerModel(modelData) {
    const {
      id,
      name,
      description,
      developerAddress,
      onnxPath,
      inputType,
      outputType,
      maxInputSize,
      price
    } = modelData;

    // Calculate ONNX hash
    const onnxHash = await this.hashFile(onnxPath);

    // Get file size
    const fs = await import('fs/promises');
    const stats = await fs.stat(onnxPath);
    const sizeMB = (stats.size / (1024 * 1024)).toFixed(2);

    await query(
      `INSERT INTO models
       (id, name, description, developer_address, onnx_path, onnx_hash, onnx_size_mb,
        input_type, output_type, max_input_size, price, status)
       VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12)`,
      [
        id, name, description, developerAddress, onnxPath, onnxHash, sizeMB,
        inputType, outputType, maxInputSize, price, 'active'
      ]
    );

    logger.info(`‚úÖ Model registered: ${id}`);

    return { id, onnxHash };
  }

  /**
   * Get model by ID
   */
  async getModel(modelId) {
    const result = await query(
      'SELECT * FROM models WHERE id = $1 AND status = $2',
      [modelId, 'active']
    );

    if (result.rows.length === 0) {
      throw new Error(`Model not found: ${modelId}`);
    }

    return result.rows[0];
  }

  /**
   * List all models
   */
  async listModels(filters = {}) {
    let sql = 'SELECT * FROM models WHERE status = $1';
    const params = ['active'];
    let paramIndex = 2;

    if (filters.category) {
      sql += ` AND category = $${paramIndex}`;
      params.push(filters.category);
      paramIndex++;
    }

    if (filters.maxPrice) {
      sql += ` AND price <= $${paramIndex}`;
      params.push(filters.maxPrice);
      paramIndex++;
    }

    sql += ' ORDER BY monthly_calls DESC';

    const result = await query(sql, params);
    return result.rows;
  }

  /**
   * Update model statistics
   */
  async updateStats(modelId, inferenceMs, proofMs, totalMs) {
    await query(
      `UPDATE models SET
       total_calls = total_calls + 1,
       monthly_calls = monthly_calls + 1,
       avg_inference_ms = COALESCE(
         (avg_inference_ms * (total_calls - 1) + $2) / total_calls,
         $2
       ),
       avg_proof_ms = COALESCE(
         (avg_proof_ms * (total_calls - 1) + $3) / total_calls,
         $3
       ),
       avg_total_ms = COALESCE(
         (avg_total_ms * (total_calls - 1) + $4) / total_calls,
         $4
       ),
       updated_at = NOW()
       WHERE id = $1`,
      [modelId, inferenceMs, proofMs, totalMs]
    );
  }

  /**
   * Calculate file hash (SHA3-256)
   */
  async hashFile(filePath) {
    const fs = await import('fs/promises');
    const fileBuffer = await fs.readFile(filePath);
    return '0x' + crypto.createHash('sha3-256').update(fileBuffer).digest('hex');
  }
}
```

**Seed Initial Models:**
```javascript
// scripts/seed-models.js
import { ModelRegistry } from '../src/services/ModelRegistry.js';

const registry = new ModelRegistry();

// Seed crypto sentiment model
await registry.registerModel({
  id: 'crypto-sentiment-v2',
  name: 'Crypto Sentiment Classifier',
  description: 'Classify crypto news as bullish/bearish/neutral',
  developerAddress: '0x1234567890123456789012345678901234567890',
  onnxPath: './models/crypto-sentiment-v2/model.onnx',
  inputType: 'text',
  outputType: 'classification',
  maxInputSize: 60,
  price: '0.25'
});

console.log('‚úÖ Models seeded');
```

---

## Week 2: ONNX Inference + JOLT Proofs

### Day 1-2: ONNX Inference Engine

**Goal:** Run ONNX models and get outputs

#### `src/services/OnnxInferenceEngine.js`
```javascript
import ort from 'onnxruntime-node';
import { logger } from '../utils/logger.js';
import { ModelRegistry } from './ModelRegistry.js';

export class OnnxInferenceEngine {
  constructor() {
    this.sessions = new Map(); // Cache loaded models
    this.registry = new ModelRegistry();
  }

  /**
   * Load model into memory
   */
  async loadModel(modelId) {
    if (this.sessions.has(modelId)) {
      return this.sessions.get(modelId);
    }

    const model = await this.registry.getModel(modelId);

    logger.info(`üì¶ Loading ONNX model: ${modelId}`);
    const session = await ort.InferenceSession.create(model.onnx_path);

    this.sessions.set(modelId, { session, model });
    logger.info(`‚úÖ Model loaded: ${modelId}`);

    return { session, model };
  }

  /**
   * Run inference
   */
  async infer(modelId, inputs) {
    const { session, model } = await this.loadModel(modelId);

    // Preprocess inputs based on model type
    const processedInputs = await this.preprocessInputs(inputs, model);

    // Create input tensors
    const feeds = {};
    const inputNames = session.inputNames;

    feeds[inputNames[0]] = new ort.Tensor(
      'float32',
      processedInputs,
      [1, processedInputs.length]
    );

    // Run inference
    const startTime = Date.now();
    const results = await session.run(feeds);
    const inferenceTime = Date.now() - startTime;

    // Extract outputs
    const outputName = session.outputNames[0];
    const outputs = Array.from(results[outputName].data);

    logger.info(`‚úÖ Inference complete: ${inferenceTime}ms`);

    return {
      outputs,
      inferenceTimeMs: inferenceTime,
      modelId,
      inputHash: this.hashInputs(inputs),
      outputHash: this.hashOutputs(outputs)
    };
  }

  /**
   * Preprocess inputs based on model type
   */
  async preprocessInputs(inputs, model) {
    // This would vary by model type
    // For now, assume feature vectors
    return Array.isArray(inputs) ? inputs : [inputs];
  }

  /**
   * Hash inputs for proof binding
   */
  hashInputs(inputs) {
    const crypto = await import('crypto');
    return '0x' + crypto.createHash('sha3-256')
      .update(JSON.stringify(inputs))
      .digest('hex');
  }

  /**
   * Hash outputs for proof binding
   */
  hashOutputs(outputs) {
    const crypto = await import('crypto');
    return '0x' + crypto.createHash('sha3-256')
      .update(JSON.stringify(outputs))
      .digest('hex');
  }
}
```

### Day 3-5: JOLT Proof Generator

**Goal:** Generate JOLT proofs for inference

#### `src/services/JoltProofGenerator.js`
```javascript
import { spawn } from 'child_process';
import { promises as fs } from 'fs';
import crypto from 'crypto';
import { config } from '../config.js';
import { logger } from '../utils/logger.js';

export class JoltProofGenerator {
  /**
   * Generate JOLT proof for inference
   */
  async generateProof(inferenceResult) {
    logger.info('üîê Generating JOLT proof...');
    const startTime = Date.now();

    try {
      // Check if JOLT binary exists
      await fs.access(config.jolt.binaryPath, fs.constants.X_OK);
    } catch {
      logger.warn(`‚ö†Ô∏è  JOLT binary not found, simulating proof`);
      return this.simulateProof(inferenceResult, startTime);
    }

    return new Promise((resolve, reject) => {
      const args = ['profile', '--name', inferenceResult.modelId];

      const joltProcess = spawn(config.jolt.binaryPath, args);

      let stdout = '';
      let stderr = '';

      joltProcess.stdout.on('data', (data) => {
        stdout += data.toString();
      });

      joltProcess.stderr.on('data', (data) => {
        stderr += data.toString();
        logger.info(`   JOLT: ${data.toString().trim()}`);
      });

      joltProcess.on('close', (code) => {
        const duration = Date.now() - startTime;

        if (code === 0) {
          logger.info(`‚úÖ JOLT proof generated in ${duration}ms`);

          const proofData = {
            modelHash: inferenceResult.modelId,
            inputHash: inferenceResult.inputHash,
            outputHash: inferenceResult.outputHash,
            verified: true,
            timestamp: Date.now()
          };

          const proofBytes = Buffer.from(JSON.stringify(proofData));
          const proofHash = '0x' + crypto.createHash('sha256')
            .update(proofBytes)
            .digest('hex');

          resolve({
            proofData: '0x' + proofBytes.toString('hex'),
            proofHash,
            duration,
            verified: true
          });
        } else {
          logger.warn(`‚ö†Ô∏è  JOLT proof failed with code ${code}`);
          resolve(this.simulateProof(inferenceResult, startTime));
        }
      });

      // Timeout
      setTimeout(() => {
        joltProcess.kill();
        logger.warn('‚ö†Ô∏è  JOLT proof timeout');
        resolve(this.simulateProof(inferenceResult, startTime));
      }, config.jolt.timeout);
    });
  }

  /**
   * Simulate proof when JOLT unavailable
   */
  async simulateProof(inferenceResult, startTime) {
    logger.warn('‚ö†Ô∏è  Using simulated JOLT proof');

    const proofData = {
      modelHash: inferenceResult.modelId,
      inputHash: inferenceResult.inputHash,
      outputHash: inferenceResult.outputHash,
      verified: false,
      timestamp: Date.now(),
      note: 'SIMULATED (not cryptographically secure)'
    };

    const proofHash = '0x' + crypto.createHash('sha256')
      .update(JSON.stringify(proofData))
      .digest('hex');

    return {
      proofData: '0x' + Buffer.from(JSON.stringify(proofData)).toString('hex'),
      proofHash,
      duration: Date.now() - startTime,
      verified: false
    };
  }
}
```

---

## Week 3: Groth16 Wrapper + API

### Day 1-2: Groth16 Wrapper Service

**Goal:** Wrap JOLT proofs in Groth16 for portability

(Reuse zkml-erc8004's joltGroth16Wrapper.js - copy and adapt)

### Day 3-5: Core API Endpoints

**Goal:** Complete inference API

#### `src/api/infer.js`
```javascript
import express from 'express';
import { OnnxInferenceEngine } from '../services/OnnxInferenceEngine.js';
import { JoltProofGenerator } from '../services/JoltProofGenerator.js';
import { Groth16WrapperService } from '../services/Groth16WrapperService.js';
import { X402PaymentVerifier } from '../services/X402PaymentVerifier.js';
import { ModelRegistry } from '../services/ModelRegistry.js';
import { query } from '../database/db.js';
import { logger } from '../utils/logger.js';

const router = express.Router();

const inferenceEngine = new OnnxInferenceEngine();
const joltProver = new JoltProofGenerator();
const groth16Wrapper = new Groth16WrapperService();
const paymentVerifier = new X402PaymentVerifier();
const modelRegistry = new ModelRegistry();

router.post('/infer/:modelId', async (req, res) => {
  try {
    const { modelId } = req.params;
    const { inputs, paymentTxHash, returnProof = true, async = false } = req.body;

    logger.info(`\\nüì∞ Inference request: ${modelId}`);

    // 1. Get model metadata
    const model = await modelRegistry.getModel(modelId);

    // 2. Verify payment
    const payment = await paymentVerifier.verifyPayment(paymentTxHash, model.price);

    if (!payment.valid) {
      return res.status(402).json({
        error: 'Payment required',
        code: 402,
        details: payment.error,
        payment: {
          amount: model.price,
          currency: 'USDC',
          recipient: config.blockchain.platformWallet,
          network: 'Base',
          chainId: 8453
        }
      });
    }

    // 3. Run inference
    const inference = await inferenceEngine.infer(modelId, inputs);

    // 4. Generate JOLT proof
    const joltProof = await joltProver.generateProof(inference);

    // 5. Wrap in Groth16
    const groth16Proof = await groth16Wrapper.wrapProof(joltProof);

    // 6. Store verification record
    const verificationId = await this.storeVerification({
      modelId,
      txHash: paymentTxHash,
      payment,
      inference,
      joltProof,
      groth16Proof
    });

    // 7. Mark payment as used
    await paymentVerifier.markUsed(paymentTxHash, verificationId);

    // 8. Update model stats
    await modelRegistry.updateStats(
      modelId,
      inference.inferenceTimeMs,
      joltProof.duration,
      groth16Proof.duration
    );

    // 9. Return result
    res.json({
      success: true,
      verificationId,
      model: {
        id: modelId,
        name: model.name
      },
      inference: {
        outputs: inference.outputs,
        timeMs: inference.inferenceTimeMs
      },
      proof: returnProof ? {
        joltProofHash: joltProof.proofHash,
        groth16Proof: groth16Proof.proof,
        publicSignals: groth16Proof.publicSignals,
        proofIpfs: groth16Proof.ipfs,
        verified: joltProof.verified
      } : undefined,
      payment: {
        txHash: paymentTxHash,
        amount: payment.amount,
        from: payment.from,
        verifiedAt: payment.timestamp
      },
      timing: {
        inference: inference.inferenceTimeMs,
        joltProof: joltProof.duration,
        groth16Wrapper: groth16Proof.duration,
        total: inference.inferenceTimeMs + joltProof.duration + groth16Proof.duration
      }
    });

  } catch (error) {
    logger.error('Inference error:', error);
    res.status(500).json({
      error: 'Inference failed',
      message: error.message
    });
  }
});

async function storeVerification(data) {
  const result = await query(
    `INSERT INTO verifications
     (model_id, tx_hash, payment_amount, payer_address,
      input_data, input_hash, output_data, output_hash,
      jolt_proof_hash, groth16_proof_ipfs,
      inference_time_ms, jolt_time_ms, groth16_time_ms, total_time_ms,
      status)
     VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15)
     RETURNING id`,
    [
      data.modelId,
      data.txHash,
      data.payment.amount,
      data.payment.from,
      JSON.stringify(data.inference.inputs),
      data.inference.inputHash,
      JSON.stringify(data.inference.outputs),
      data.inference.outputHash,
      data.joltProof.proofHash,
      data.groth16Proof.ipfs,
      data.inference.inferenceTimeMs,
      data.joltProof.duration,
      data.groth16Proof.duration,
      data.inference.inferenceTimeMs + data.joltProof.duration + data.groth16Proof.duration,
      'success'
    ]
  );

  return result.rows[0].id;
}

export { router as inferRouter };
```

---

## Week 4: Deployment + Testing

### Day 1-2: Service Manifest + Discovery

Create /.well-known/ai-service.json (copy from planning doc)

### Day 3: Testing

```bash
# Unit tests
npm test

# Integration test
npm run test:integration

# Load test (100 concurrent requests)
npm run test:load
```

### Day 4-5: Deploy to Production

```bash
# Deploy to Render.com
render-deploy ./

# Configure DNS
# Set environment variables
# Monitor with Sentry
```

---

## Next Steps After MVP

1. **Add more models** (token risk, fraud detector)
2. **Developer portal** (submission UI + analytics)
3. **IPFS integration** (permanent proof storage)
4. **Async inference** (job queue with Bull)
5. **Rate limiting** (Redis)
6. **Documentation site** (API reference + examples)
7. **X402 bazaar listing** (submit service manifest)

---

## Success Criteria

**MVP is complete when:**
- ‚úÖ Single model (crypto-sentiment-v2) is live
- ‚úÖ Payment verification works (Base + USDC)
- ‚úÖ JOLT proofs generate successfully
- ‚úÖ Groth16 wrapper creates portable proofs
- ‚úÖ API returns verified inference results
- ‚úÖ Service manifest available at /.well-known/ai-service.json
- ‚úÖ 10 test inferences completed with real payments
- ‚úÖ Total revenue: $2.50 (10 √ó $0.25)

**Ready for Phase 2 when:**
- ‚úÖ Model registry supports multiple models
- ‚úÖ Developer submission API works
- ‚úÖ Revenue tracking + payouts implemented
- ‚úÖ 3 models live (sentiment + risk + fraud)
- ‚úÖ 2 developer submissions accepted
- ‚úÖ $500/month revenue achieved
